\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}

\begin{document}

\title{Prediction as Action:\\When RL Gradients Replace Pretraining}

\author{\IEEEauthorblockN{Vivek Prashant Padman}
\IEEEauthorblockA{\textit{DayDreamersAI} \\
Pune, India \\
vivekp.padman@gmail.com}
}

\maketitle

\begin{abstract}
We investigate whether reinforcement learning can replace pretraining for learning predictive representations. Through systematic experiments, we first establish a negative result: world models trained as auxiliary prediction losses are ``causally inert''---they predict dynamics accurately but never influence action selection, and can actively degrade performance. Model-based value expansion (MVE), which queries the world model for planning, also fails when value estimates are untrained. However, we discover a positive result when we \textbf{unify prediction and action}: by treating the predicted next state as the action itself and rewarding prediction accuracy, RL gradients successfully train predictive models to 100\% accuracy---matching supervised learning (MLE) on final performance. This demonstrates that \textit{RL reward signals can shape representations for prediction as effectively as likelihood-based losses}. Our key insight: pretraining is not replaced by ``using world models for planning''---it is replaced when prediction itself becomes the control objective. This reframes the relationship between learning and using, suggesting that the architectural separation between prediction and control is what makes pretraining seem necessary.
\end{abstract}

\begin{IEEEkeywords}
world models, reinforcement learning, prediction, pretraining, representation learning
\end{IEEEkeywords}

\section{Introduction}

The dominant paradigm for learning predictive representations is pretraining: optimizing a likelihood-based objective (cross-entropy, MSE) over large datasets before fine-tuning for downstream tasks. This approach underlies modern language models \cite{brown2020gpt3}, vision transformers \cite{dosovitskiy2021vit}, and world models for RL \cite{hafner2023dreamerv3}.

We ask a fundamental question: \textbf{Is pretraining necessary for learning accurate predictions, or can reinforcement learning alone achieve the same result?}

This question is motivated by biological learning. Human infants develop sophisticated predictive models through interaction---not by minimizing cross-entropy over sensory streams, but by acting, observing outcomes, and receiving scalar feedback (success, failure, surprise). If intelligence can emerge this way, the architectural requirements for prediction learning may be more flexible than current practice suggests.

\subsection{Summary of Contributions}

We present a progression of experiments that establish both negative and positive results:

\textbf{Negative Result 1: Auxiliary world models are causally inert.} Training a world-model head alongside a policy head with shared representations does not enable planning. The world model predicts accurately (MSE 0.05) but the policy never queries it. Performance is \textit{worse} than no world model at all (23\% vs 50\% on a navigation task).

\textbf{Negative Result 2: Model-based value expansion fails without trained values.} Querying the world model for value targets (MVE) makes predictions causally relevant, but fails when value estimates are untrained---adding noise that hurts learning (17\% vs 28\%).

\textbf{Positive Result: Prediction-as-action succeeds.} When we unify prediction and control---making the predicted next state the ``action'' and rewarding accuracy---RL gradients train the model to 100\% prediction accuracy, matching supervised MLE.

Our core finding:

\begin{quote}
\textit{Pretraining is not replaced by ``using world models.'' Pretraining is replaced when prediction itself becomes the control objective.}
\end{quote}

\subsection{Why This Matters}

The positive result is not obvious. RL gradients are high-variance, there is no teacher forcing, credit assignment is indirect, and the reward is a scalar summary of prediction quality. Yet the system converges reliably to perfect accuracy.

This demonstrates that the \textbf{optimization dynamics} of RL are sufficient for prediction learning---not just the \textbf{objective alignment}. The key architectural insight is that separating prediction (auxiliary loss) from control (policy) is what makes pretraining seem necessary. When unified, RL alone suffices.

\section{Background and Related Work}

\subsection{World Models in Reinforcement Learning}

World models learn to predict environmental dynamics, enabling agents to plan ahead. Sutton's Dyna \cite{sutton1991dyna} pioneered model-based RL by using learned models for simulated experience. Modern approaches like Dreamer \cite{hafner2020dreamer, hafner2023dreamerv3} train policies entirely in imagination, while MuZero \cite{schrittwieser2020muzero} combines learned models with tree search.

Crucially, these successful approaches \textit{actively use} the world model during policy training. Our experiments isolate what happens when world models are learned but not used.

\subsection{Auxiliary Prediction Losses}

Using prediction as an auxiliary task is common in deep RL. UNREAL \cite{jaderberg2017unreal} added reward and pixel prediction. Self-supervised methods \cite{laskin2020curl, schwarzer2021spr} use contrastive or predictive objectives. The claimed benefit is ``representation learning,'' but whether this enables planning is often implicit.

\subsection{Model-Based Value Expansion}

MVE \cite{feinberg2018mve} uses learned models to compute better value targets: $V_{target} = r + \gamma V(\hat{s}')$ where $\hat{s}'$ is the predicted next state. This makes the world model causally relevant to policy gradients. STEVE \cite{buckman2018steve} extends this with uncertainty-weighted horizons.

\subsection{Prediction and Control}

Active inference \cite{friston2010free, millidge2021expected} proposes that action and perception serve the same objective: minimizing prediction error. Decision Transformer \cite{chen2021dt} frames RL as sequence prediction conditioned on returns. These works blur the boundary between prediction and control, but typically still use offline data (implicit pretraining).

\subsection{Key Distinction: MLE vs RL Gradients}

\begin{table}[htbp]
\caption{Supervised MLE vs Prediction-as-Action RL}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Property} & \textbf{MLE} & \textbf{RL (Ours)} \\
\midrule
Target supervision & Teacher-forced & None \\
Gradient signal & Per-dimension & Scalar reward \\
Credit assignment & Direct & Indirect \\
Data source & Dataset & Online interaction \\
Distribution & Fixed & Policy-dependent \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Yes, the objectives align (both optimize prediction accuracy). But the optimization dynamics are fundamentally different. Our positive result shows RL dynamics suffice.

\subsection{The Evolution of World Model Training: A Loss Function Perspective}

To understand our contribution, we trace how the relationship between prediction and control has evolved through different training paradigms. Table~\ref{tab:loss_evolution} summarizes this evolution.

\begin{table*}[htbp]
\caption{Evolution of World Model Training: Loss Functions and Gradient Flow}
\label{tab:loss_evolution}
\begin{center}
\begin{tabular}{llll}
\toprule
\textbf{Paradigm} & \textbf{Loss Function} & \textbf{Gradient to WM?} & \textbf{WM Causally Used?} \\
\midrule
Auxiliary WM & $\mathcal{L} = \mathcal{L}_{RL} + \lambda \mathcal{L}_{WM}$ & Yes (from $\mathcal{L}_{WM}$) & No \\
MVE/Dyna & $\mathcal{L} = \mathcal{L}_{RL}(V(\hat{s}')) + \lambda \mathcal{L}_{WM}$ & Partial (through $V$) & Yes (for value) \\
Dreamer & $\mathcal{L} = \mathcal{L}_{RL}^{imagined} + \mathcal{L}_{WM}$ & Yes (both paths) & Yes (for rollouts) \\
\textbf{Pred-as-Action (Ours)} & $\mathcal{L} = \mathcal{L}_{RL}(r = f(\hat{s}, s'))$ & \textbf{Yes (direct)} & \textbf{Yes (IS the action)} \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}

\subsubsection{Detailed Loss Formulations}

\textbf{1. Auxiliary World Model (Standard Practice):}
\begin{align}
\mathcal{L}_{total} &= \mathcal{L}_{policy} + \lambda \mathcal{L}_{WM} \\
\mathcal{L}_{policy} &= -\mathbb{E}_{\pi}[\log \pi(a|s) \cdot A(s,a)] \\
\mathcal{L}_{WM} &= \mathbb{E}_{(s,a,s') \sim \mathcal{D}}[\|\hat{s}' - s'\|^2]
\end{align}
The world model receives MLE gradients from $\mathcal{L}_{WM}$, but the policy gradient $\nabla_\theta \mathcal{L}_{policy}$ does not flow through the world model. The WM is learned but never queried.

\textbf{2. Model-Based Value Expansion (MVE):}
\begin{align}
\mathcal{L}_{total} &= \mathcal{L}_{policy} + \mathcal{L}_{value} + \lambda \mathcal{L}_{WM} \\
V_{target}(s) &= r + \gamma V(\underbrace{WM(s,a)}_{\hat{s}'}) \\
\mathcal{L}_{value} &= \|V(s) - V_{target}(s)\|^2
\end{align}
The WM is queried to compute value targets, creating a causal path. However, RL gradients reach the WM only through the value function, and noisy $V$ estimates corrupt the signal.

\textbf{3. Dreamer (Imagination-Based):}
\begin{align}
\mathcal{L}_{total} &= \mathcal{L}_{policy}^{imagined} + \mathcal{L}_{value}^{imagined} + \mathcal{L}_{WM} \\
\mathcal{L}_{policy}^{imagined} &= -\mathbb{E}_{\hat{\tau} \sim WM}[\sum_t \gamma^t \hat{r}_t]
\end{align}
Policy is trained entirely on imagined trajectories $\hat{\tau}$ from the WM. This requires accurate WM (pretrained with $\mathcal{L}_{WM}$) before policy learning can succeed.

\textbf{4. Prediction-as-Action (Ours):}
\begin{align}
\mathcal{L}_{total} &= \mathcal{L}_{policy} \quad \text{(no auxiliary loss!)} \\
a &= \hat{s}_{t+1} \quad \text{(action IS prediction)} \\
r &= -\|\hat{s}_{t+1} - s_{t+1}\|^2 \quad \text{(reward IS accuracy)} \\
\mathcal{L}_{policy} &= -\mathbb{E}_{\pi}[\log \pi(\hat{s}|s) \cdot r]
\end{align}
There is no separate $\mathcal{L}_{WM}$---the RL objective directly optimizes prediction accuracy. The gradient path is: $r \to \log \pi \to \theta_{prediction}$.

\subsubsection{Gradient Flow Comparison}

\begin{table}[htbp]
\caption{Gradient Flow to Prediction Parameters}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Approach} & \textbf{Gradient Source} & \textbf{Signal Quality} \\
\midrule
Auxiliary WM & $\nabla \mathcal{L}_{WM}$ (MLE) & Dense, low-variance \\
MVE & $\nabla \mathcal{L}_{WM} + \nabla V$ & Mixed, noisy \\
Dreamer & $\nabla \mathcal{L}_{WM} + \nabla \mathcal{L}_{RL}^{img}$ & Dense + sparse \\
\textbf{Ours} & $\nabla \mathcal{L}_{RL}$ only & Sparse, high-variance \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Our approach is unique: prediction parameters receive \textit{only} RL gradients---no MLE supervision. Yet it achieves the same final accuracy.

\subsection{Comparison with LLM Pretraining + RLHF}

Our prediction-as-action paradigm has a striking parallel to how large language models are trained. Table~\ref{tab:llm_comparison} draws this comparison.

\begin{table*}[htbp]
\caption{Structural Comparison: LLM Training vs Our Prediction-as-Action}
\label{tab:llm_comparison}
\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{LLM (Pretraining + RLHF)} & \textbf{Prediction-as-Action (Ours)} \\
\midrule
\textbf{Action space} & Next token $\in \mathcal{V}$ (discrete) & Next state $\in \mathbb{R}^d$ (continuous) \\
\textbf{Prediction target} & $P(x_{t+1} | x_{1:t})$ & $P(s_{t+1} | s_t)$ \\
\textbf{Pretraining loss} & $\mathcal{L}_{MLE} = -\log P(x_{t+1}|x_{1:t})$ & $\mathcal{L}_{MLE} = \|s_{t+1} - \hat{s}_{t+1}\|^2$ \\
\textbf{RL reward} & $r = R_{human}(\text{response})$ & $r = -\|\hat{s}_{t+1} - s_{t+1}\|^2$ \\
\textbf{RL objective} & $\max_\theta \mathbb{E}[R_{human}] - \beta D_{KL}$ & $\max_\theta \mathbb{E}[-\text{MSE}]$ \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}

\subsubsection{LLM Training: The Two-Phase Paradigm}

Standard LLM training proceeds in two phases:

\textbf{Phase 1: Pretraining (MLE)}
\begin{align}
\mathcal{L}_{pretrain} &= -\sum_{t} \log P_\theta(x_t | x_{<t}) \\
&= \text{Cross-entropy with teacher forcing}
\end{align}
This requires massive datasets and compute, but provides dense per-token gradients.

\textbf{Phase 2: RLHF (Reinforcement Learning)}
\begin{align}
\mathcal{L}_{RLHF} &= -\mathbb{E}_{y \sim \pi_\theta}[R(y)] + \beta D_{KL}(\pi_\theta \| \pi_{ref}) \\
R(y) &= \text{Reward model score (scalar)}
\end{align}
RL fine-tunes the pretrained model using scalar human preference signals.

\textbf{The Assumed Necessity of Phase 1:} It is widely believed that Phase 1 is essential---that RL alone cannot learn language from scratch because:
\begin{itemize}
    \item RL gradients are high-variance
    \item Credit assignment across long sequences is hard
    \item The action space (vocabulary) is too large
\end{itemize}

\subsubsection{Our Result Challenges This Assumption}

We show that when the RL reward directly measures prediction accuracy (not a proxy like human preference), RL gradients alone suffice:

\begin{table}[htbp]
\caption{What If RLHF Reward = Prediction Accuracy?}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Training} & \textbf{Reward Signal} & \textbf{Can Replace Pretraining?} \\
\midrule
Standard RLHF & Human preference & No (different objective) \\
\textbf{Pred-as-Action} & Prediction accuracy & \textbf{Yes (same objective)} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

The key insight: RLHF cannot replace pretraining because it optimizes a \textit{different} objective (human preference vs prediction accuracy). Our approach uses RL to optimize the \textit{same} objective as pretraining---and succeeds.

\subsubsection{Unified Loss Function View}

\begin{table}[htbp]
\caption{Loss Functions Across Paradigms}
\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Paradigm} & \textbf{Total Loss} \\
\midrule
LLM Pretrain & $\mathcal{L} = -\sum_t \log P(x_t|x_{<t})$ \\
LLM + RLHF & $\mathcal{L} = \mathcal{L}_{pretrain} \to \mathcal{L}_{RLHF}$ (sequential) \\
Aux WM + RL & $\mathcal{L} = \mathcal{L}_{RL} + \lambda \mathcal{L}_{WM}$ (parallel) \\
\textbf{Ours} & $\mathcal{L} = \mathcal{L}_{RL}$ where $r = $ pred accuracy \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Our formulation is the simplest: a single RL loss where the reward encodes prediction accuracy. No pretraining phase, no auxiliary losses, no two-stage training.

\section{Experimental Framework}

\subsection{Tasks}

We design symbolic tasks that isolate when world models become necessary:

\textbf{NavigationTask (T=3):} An agent on a circular grid (8 positions) must navigate to a target using relative movements (action $a$ causes movement $a-3$). The agent takes 2 actions before receiving sparse reward at step 3. This requires multi-step planning.

State encoding: $s = [\text{pos}_{one-hot}(8), \text{target}_{one-hot}(8), \text{phase}_{one-hot}(3)]$ (19 dimensions).

\textbf{PredictionTask:} The agent's action IS its predicted next state (continuous 8-dim vector). Reward = $-\text{MSE}(\text{prediction}, \text{actual})$. Dynamics are deterministic (circular shift or random permutation). This unifies prediction and control.

\subsection{Model Architecture}

A shared transformer backbone (2 layers, 64 hidden, 4 heads) feeds task-specific heads:

\textbf{For NavigationTask:}
\begin{itemize}
    \item Policy head: $\pi(a|s)$ over 8 discrete actions
    \item Value head: $V(s)$ for baseline
    \item World-model head: $\hat{s}_{t+1} = f(s_t, a_t)$
\end{itemize}

\textbf{For PredictionTask:}
\begin{itemize}
    \item Prediction head: Gaussian policy $\mathcal{N}(\mu(s), \sigma)$ over predicted state
    \item Value head: $V(s)$ for baseline
    \item No separate world-model head (prediction IS the action)
\end{itemize}

\subsection{Training Algorithms}

\textbf{REINFORCE} \cite{williams1992reinforce} with learned baseline for policy gradient. For PredictionTask, we use Gaussian policy with learnable log-std.

\textbf{MLE baseline:} Direct MSE supervision on predictions (for comparison with RL).

\section{Negative Results: Learning $\neq$ Using}

\subsection{Experiment 1: Auxiliary World Model}

We train on NavigationTask with three conditions:

\begin{table}[htbp]
\caption{NavigationTask: Auxiliary WM Fails}
\begin{center}
\begin{tabular}{lcccc}
\toprule
\textbf{Condition} & \textbf{Success} & \textbf{Best} & \textbf{WM Loss} \\
\midrule
\textbf{No WM} & \textbf{50.0\%} & \textbf{59.4\%} & 0.24 \\
Frozen WM & 43.8\% & 43.8\% & 0.15 \\
Joint WM & 23.4\% & 25.0\% & 0.05 \\
\midrule
Random & 12.5\% & -- & -- \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Finding:} Removing the world-model loss entirely yields the \textit{best} performance. The world model learns accurate dynamics (loss 0.05) but this \textit{hurts} the policy.

\textbf{Analysis:} Two factors contribute:
\begin{enumerate}
    \item \textbf{Gradient interference:} WM and policy losses compete for shared representations. Frozen WM (no gradient competition) recovers from 23\% to 44\%.
    \item \textbf{Causal irrelevance:} Even frozen, the WM doesn't help---the policy never queries it. $P(a|s; WM) = P(a|s)$.
\end{enumerate}

\subsection{Experiment 2: Model-Based Value Expansion}

We implement MVE with horizon $H=1$:
\begin{equation}
V_{target}(s_t) = r_t + \gamma \cdot V(\hat{s}_{t+1})
\end{equation}
where $\hat{s}_{t+1} = WM(s_t, a_t)$ is the predicted next state.

\begin{table}[htbp]
\caption{MVE H=1: Querying WM Still Fails}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Condition} & \textbf{Final} & \textbf{Best} & \textbf{WM Loss} \\
\midrule
No WM (baseline) & 28.1\% & 39.1\% & 0.24 \\
\textbf{MVE H=1} & \textbf{17.2\%} & \textbf{26.6\%} & 0.057 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Finding:} MVE makes the WM causally relevant (value targets depend on predictions), but performance is \textit{worse} than the baseline.

\textbf{Analysis:} MVE requires accurate \textit{value} estimates, not just accurate \textit{dynamics}. Our REINFORCE uses an EMA baseline, not a trained value function. The value head outputs are essentially random, so $\gamma \cdot V(\hat{s}')$ adds noise that corrupts advantage estimates.

\textbf{Implication:} ``Querying the world model'' is necessary but not sufficient. The query must produce useful information. This validates that we need a more fundamental architectural change.

\section{Positive Result: Prediction as Action}

This section presents our core contribution: demonstrating that RL gradients can replace MLE for learning predictive representations when prediction is treated as the action itself.

\subsection{The Key Architectural Insight}

All previous approaches in this paper---and in much of model-based RL---maintain a fundamental separation:

\begin{itemize}
    \item Prediction is an \textit{auxiliary} task (trained by MLE/MSE loss)
    \item Control is the \textit{primary} objective (trained by RL reward)
    \item These are separate heads with separate gradients
\end{itemize}

We hypothesize that \textbf{this separation is what makes pretraining seem necessary}. When prediction is auxiliary, it must be trained by supervised loss because the RL reward doesn't reach it.

Our solution: \textbf{remove the separation entirely}.

\begin{quote}
\textit{The agent's action IS its prediction of the next state.\\
The environment's reward IS prediction accuracy.\\
There is no auxiliary loss---prediction is the control objective.}
\end{quote}

This mirrors the structure of language modeling:
\begin{align}
\text{LM Action} &= \text{predict next token} \\
\text{LM ``Reward''} &= \log P(\text{correct token})
\end{align}

The difference: standard LM training uses MLE gradients (teacher forcing). We ask: \textbf{can RL gradients achieve the same result?}

\subsection{PredictionTask: Experimental Design}

We design a minimal task that isolates the question:

\begin{table}[htbp]
\caption{PredictionTask Specification}
\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
State space & One-hot position vector (8 dims) \\
Action space & Continuous prediction (8 dims) \\
Dynamics & Deterministic transition function \\
Reward & $r = -\text{MSE}(\hat{s}_{t+1}, s_{t+1})$ \\
Episode length & $T=1$ (single-step) \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Dynamics types tested:}
\begin{itemize}
    \item \textbf{Circular shift:} $s_{t+1} = (s_t + 1) \mod 8$. Simple, learnable pattern.
    \item \textbf{Random permutation:} Fixed but arbitrary mapping. Requires memorizing 8 transitions.
\end{itemize}

\textbf{Model architecture:} Same transformer backbone as NavigationTask (2 layers, 64 hidden, 4 heads, $\sim$111K parameters). The prediction head outputs Gaussian parameters $(\mu, \sigma)$ for continuous actions.

\subsection{Training Protocols: RL vs MLE}

We compare two training protocols on \textit{identical architectures}:

\begin{table}[htbp]
\caption{Training Protocol Comparison}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Property} & \textbf{RL (REINFORCE)} & \textbf{MLE (Supervised)} \\
\midrule
Loss function & $-\log \pi(\hat{s}|s) \cdot A$ & $\|\hat{s} - s_{t+1}\|^2$ \\
Target access & None (scalar reward) & Full target vector \\
Gradient type & Policy gradient & Direct backprop \\
Exploration & Gaussian noise $\sigma$ & None needed \\
Credit assignment & Through reward & Per-dimension \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

This comparison is critical: if RL matches MLE, it demonstrates that \textbf{the optimization dynamics of RL are sufficient for prediction learning}, not just that the objectives align.

\subsection{Main Results}

\begin{table}[htbp]
\caption{Prediction-as-Action: RL Matches MLE on Final Accuracy}
\begin{center}
\begin{tabular}{llcccc}
\toprule
\textbf{Dynamics} & \textbf{Training} & \textbf{Final Acc} & \textbf{Final MSE} & \textbf{Best Acc} & \textbf{Steps to 100\%} \\
\midrule
\multirow{2}{*}{Circular} & RL & \textbf{100\%} & 0.020 & 100\% & $\sim$8,000 \\
 & MLE & \textbf{100\%} & 0.000 & 100\% & $\sim$1,600 \\
\midrule
\multirow{2}{*}{Random} & RL & \textbf{100\%} & 0.027 & 100\% & $\sim$8,000 \\
 & MLE & \textbf{100\%} & 0.000 & 100\% & $\sim$800 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Key observations:}

\begin{enumerate}
    \item \textbf{RL achieves 100\% accuracy.} This is the core result. RL gradients successfully train the model to perfectly predict dynamics.

    \item \textbf{RL matches MLE on final performance.} Both achieve the same ceiling---perfect prediction---demonstrating that RL is sufficient.

    \item \textbf{MLE is $\sim$5$\times$ faster.} Direct supervision is more sample-efficient. This is expected: MLE provides 8-dimensional gradient signal per step; RL provides only a scalar reward.

    \item \textbf{RL generalizes across dynamics.} Both circular (structured) and random (unstructured) dynamics are learned, showing robustness.
\end{enumerate}

\subsection{Learning Dynamics Analysis}

To understand \textit{how} RL learns prediction, we analyze the training trajectories:

\begin{table}[htbp]
\caption{Learning Trajectory Comparison (Circular Shift)}
\begin{center}
\begin{tabular}{lcccc}
\toprule
\textbf{Training Step} & \multicolumn{2}{c}{\textbf{RL}} & \multicolumn{2}{c}{\textbf{MLE}} \\
 & Acc & MSE & Acc & MSE \\
\midrule
1,000 & 15.6\% & 1.26 & 100\% & 0.006 \\
2,000 & 18.8\% & 0.90 & 100\% & 0.000 \\
4,000 & 46.0\% & 0.08 & 100\% & 0.000 \\
6,000 & 80.0\% & 0.04 & 100\% & 0.000 \\
8,000 & 100\% & 0.04 & 100\% & 0.000 \\
10,000 & 100\% & 0.02 & 100\% & 0.000 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Analysis:}
\begin{itemize}
    \item \textbf{MLE converges immediately:} Direct supervision provides dense signal; the model learns the mapping in $\sim$1000 steps.
    \item \textbf{RL shows exploration phase:} Early training has low accuracy as the policy explores prediction space via Gaussian noise.
    \item \textbf{RL shows sudden improvement:} Around step 4000, accuracy jumps as the policy commits to correct predictions.
    \item \textbf{RL continues refining:} MSE decreases even after 100\% accuracy, as the policy sharpens predictions.
\end{itemize}

\subsection{The Core Empirical Finding}

\begin{quote}
\textbf{RL reward signals can shape representations for prediction as effectively as MLE loss.}
\end{quote}

This is the central result of our paper. It has three implications:

\begin{enumerate}
    \item \textbf{Pretraining is not fundamentally necessary.} The same final accuracy is achievable with RL alone.

    \item \textbf{The efficiency gap is real but not prohibitive.} RL requires $\sim$5$\times$ more samples, but converges reliably.

    \item \textbf{The architectural unification is key.} When prediction = action, RL gradients reach the prediction head directly.
\end{enumerate}

\subsection{Why This Result Is Not Trivial}

A reviewer might object: ``Isn't this just supervised learning with a weird loss function?''

\textbf{No.} The optimization dynamics are fundamentally different:

\begin{table}[htbp]
\caption{Why RL $\neq$ ``Weird Supervised Learning''}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Challenge} & \textbf{MLE} & \textbf{RL} \\
\midrule
Teacher forcing & Yes (sees target) & No \\
Gradient dimensionality & $d$ (state dim) & $1$ (scalar reward) \\
Exploration required & No & Yes (policy noise) \\
Distribution shift & No (fixed data) & Yes (policy-dependent) \\
Variance & Low & High \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

RL faces all of these challenges and \textit{still succeeds}. This is the substantive finding.

\subsection{Ablation: Reward Shaping}

We test whether dense reward ($-$MSE) is necessary, or if binary reward suffices:

\begin{table}[htbp]
\caption{Reward Type Ablation}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Reward Type} & \textbf{Final Accuracy} & \textbf{Steps to 100\%} \\
\midrule
Dense ($-$MSE) & 100\% & $\sim$8,000 \\
Binary (1 if MSE $<$ 0.1) & 100\% & $\sim$12,000 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Finding:} Even binary reward works, though slower. This suggests RL can learn prediction even with sparse feedback---closer to how biological learning might operate.

\subsection{Ablation: Delayed Reward}

A key challenge for RL is credit assignment: can the system learn when reward is delayed? We test by requiring the agent to make a sequence of predictions before receiving cumulative feedback.

\begin{table}[htbp]
\caption{Delayed Reward Ablation: Credit Assignment Challenge}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Delay (steps)} & \textbf{Final Accuracy} & \textbf{Best Accuracy} & \textbf{Final MSE} \\
\midrule
1 (immediate) & 100.0\% & 100.0\% & 0.021 \\
2 & 87.0\% & 87.0\% & 0.040 \\
3 & 75.7\% & 75.7\% & 0.071 \\
5 & 34.7\% & 38.2\% & 0.109 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Findings:}
\begin{enumerate}
    \item \textbf{Immediate reward achieves 100\%.} This is our main result---RL matches MLE when feedback is immediate.
    \item \textbf{Accuracy degrades with delay.} As expected, credit assignment becomes harder with delayed feedback.
    \item \textbf{Moderate delay (2-3 steps) still achieves 75-87\%.} The system partially succeeds even with delayed feedback.
    \item \textbf{Long delay (5 steps) significantly impairs learning.} This identifies a limitation of the current approach.
\end{enumerate}

\textbf{Implication:} The success of prediction-as-action depends on timely feedback. This parallels LLM training where per-token feedback (MLE) is more efficient than sequence-level feedback (RL). Future work could explore temporal difference methods to bridge this gap.

\subsection{Ablation: Representation Similarity (RL vs MLE)}

Do RL and MLE learn similar representations? We compare the hidden representations of models trained with each method using linear probes and Canonical Correlation Analysis (CCA).

\begin{table}[htbp]
\caption{Representation Comparison: RL vs MLE}
\begin{center}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Linear Probe Accuracy (RL) & 100\% \\
Linear Probe Accuracy (MLE) & 100\% \\
\midrule
CCA Similarity (RL vs MLE) & 0.878 \\
CCA Similarity (RL vs Random) & 0.321 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Findings:}
\begin{enumerate}
    \item \textbf{Both representations are fully linearly decodable.} Position information is preserved in both RL and MLE representations.
    \item \textbf{High CCA similarity (0.878).} RL and MLE learn representations that are highly correlated, far above the random baseline (0.321).
    \item \textbf{RL representations are not degenerate.} Despite high-variance gradients, RL learns structured representations comparable to MLE.
\end{enumerate}

\textbf{Implication:} RL gradients shape representations similarly to MLE gradients when optimizing the same objective. The learning algorithm matters less than the objective alignment.

\subsection{Comparison: All Approaches}

We now compare prediction-as-action against all previous approaches on the same underlying question: can RL enable accurate prediction?

\begin{table}[htbp]
\caption{Complete Comparison: Learning Prediction Accuracy}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Approach} & \textbf{Prediction Acc} & \textbf{Causal?} & \textbf{Works?} \\
\midrule
MLE (pretraining) & 100\% & N/A & Yes (baseline) \\
\midrule
Auxiliary WM (joint) & N/A$^\dagger$ & No & No \\
Auxiliary WM (frozen) & N/A$^\dagger$ & No & No \\
MVE H=1 & N/A$^\dagger$ & Partial & No \\
\midrule
\textbf{Pred-as-Action (RL)} & \textbf{100\%} & \textbf{Yes} & \textbf{Yes} \\
\bottomrule
\end{tabular}
\end{center}
\footnotesize{$^\dagger$These approaches don't directly measure prediction accuracy as the task objective.}
\end{table}

\textbf{The pattern is clear:} Auxiliary approaches fail because prediction is not the control objective. When we make prediction the action, RL succeeds---matching MLE.

\subsection{What This Proves About Pretraining}

Our result provides evidence for a specific claim:

\begin{quote}
\textit{Pretraining (MLE) is necessary when prediction and control are architecturally separated. When unified, RL suffices.}
\end{quote}

This reframes the debate about pretraining:

\begin{itemize}
    \item \textbf{Standard view:} ``RL cannot learn prediction; MLE is needed for representation learning.''
    \item \textbf{Our finding:} ``RL \textit{can} learn prediction when prediction IS the objective, not auxiliary.''
\end{itemize}

The architectural choice---not the learning algorithm---determines whether pretraining is needed.

\section{Discussion}

\subsection{The Unification Principle}

Our experiments reveal a unifying principle across all conditions:

\begin{quote}
\textit{Prediction accuracy improves when---and only when---prediction is causally connected to the optimization objective in a way that provides useful gradient signal.}
\end{quote}

\begin{table}[htbp]
\caption{Summary: The Causal Relevance Hierarchy}
\begin{center}
\begin{tabular}{lccl}
\toprule
\textbf{Approach} & \textbf{Causal?} & \textbf{Works?} & \textbf{Failure Mode} \\
\midrule
Auxiliary WM & No & No & Never queried \\
MVE H=1 & Partial & No & Noisy value estimates \\
\textbf{Pred-as-Action} & \textbf{Full} & \textbf{Yes} & \textbf{None} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Causal relevance is necessary but not sufficient. The connection must be \textit{informative}---providing useful gradient signal. MVE fails because the value estimates are noise. Prediction-as-action succeeds because the reward directly measures prediction quality.

\subsection{Reframing Pretraining}

Our results lead to a precise reframing of when pretraining is necessary:

\begin{table}[htbp]
\caption{When Is Pretraining Necessary?}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Architecture} & \textbf{Prediction-Control Relation} & \textbf{MLE Required?} \\
\midrule
Standard (separate heads) & Auxiliary & Yes \\
MVE (value expansion) & Tool for planning & Partially \\
\textbf{Pred-as-Action (unified)} & \textbf{Identical} & \textbf{No} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{The insight:} Pretraining is not a fundamental requirement for prediction learning---it is an artifact of architectural choices that separate prediction from control.

\subsection{Connection to Biological Learning}

Human infants don't minimize cross-entropy over sensory streams. They:
\begin{itemize}
    \item Act in the world
    \item Predict outcomes
    \item Receive scalar feedback (surprise, success, failure)
\end{itemize}

Our prediction-as-action architecture mirrors this structure. The agent predicts, the environment judges, and learning proceeds through reward. This is no longer philosophy---it's a concrete architecture that works.

\subsection{Connection to Language Modeling}

Language modeling already treats prediction as action:
\begin{align}
\text{Action} &= \text{predict next token} \\
\text{Reward} &= \log P(\text{correct token})
\end{align}

The difference is that standard LM training uses MLE gradients (teacher forcing, per-token cross-entropy). Our result suggests RL gradients could work too---slower, but converging to the same representations.

This bridges to RLHF \cite{ouyang2022instructgpt}: the model is pretrained with MLE, then fine-tuned with RL. Our result suggests the pretraining phase may not be strictly necessary if the RL signal is sufficiently informative about prediction quality.

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Scale:} Our experiments use 8-dim state spaces and $\sim$100K parameters. Scaling to language-sized models requires further investigation.

    \item \textbf{Sample efficiency:} MLE converges 5$\times$ faster. For large-scale training, this efficiency gap matters.

    \item \textbf{Credit assignment with delay:} Our ablation shows accuracy drops from 100\% (immediate) to 35\% (5-step delay). Temporal difference methods may help.

    \item \textbf{Partial observability:} Our tasks are fully observable. Extending to POMDPs requires memory mechanisms.
\end{enumerate}

\subsection{Future Directions}

\begin{enumerate}
    \item \textbf{Multi-step prediction:} Extend to sequence prediction (multiple tokens/states).

    \item \textbf{Representation analysis:} Compare hidden representations learned by RL vs MLE (CCA, linear probes).

    \item \textbf{Delayed reward:} Test whether RL still converges with sparse/delayed prediction feedback.

    \item \textbf{Scaling:} Apply to larger state spaces and longer sequences.

    \item \textbf{Language:} Design text-based prediction tasks where action = predicted next token.
\end{enumerate}

\section{Conclusion}

We investigated whether reinforcement learning can replace pretraining for learning predictive representations. Through systematic experiments, we established:

\begin{enumerate}
    \item \textbf{Negative:} Auxiliary world models are causally inert and can hurt performance.
    \item \textbf{Negative:} Model-based value expansion fails without trained value estimates.
    \item \textbf{Positive:} When prediction IS the action, RL matches MLE on final accuracy.
\end{enumerate}

Our core finding: \textit{Pretraining is not replaced by ``using world models.'' It is replaced when prediction itself becomes the control objective.}

This reframes the relationship between learning and using. The architectural separation between prediction and control is what makes pretraining seem necessary. When unified under a single RL objective, prediction accuracy emerges from interaction alone.

For researchers pursuing learning from interaction, our results suggest that the critical design choice is not whether to use a world model, but whether prediction is the objective or merely auxiliary. When prediction is primary, RL suffices.

\section*{Code Availability}

Our experimental framework is available at:\\
\texttt{https://github.com/vieveks/self\_supervised\_world\_model\_learning\_with\_synthetic\_interactive\_feedback}

\section*{Acknowledgment}

We thank the anonymous reviewers for their valuable feedback.

\begin{thebibliography}{00}

% Language Models and Pretraining
\bibitem{brown2020gpt3} T. Brown et al., ``Language Models are Few-Shot Learners,'' in NeurIPS, 2020.

\bibitem{dosovitskiy2021vit} A. Dosovitskiy et al., ``An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,'' in ICLR, 2021.

% World Models and Model-Based RL
\bibitem{sutton1991dyna} R. S. Sutton, ``Dyna, an integrated architecture for learning, planning, and reacting,'' SIGART Bulletin, vol. 2, no. 4, pp. 160--163, 1991.

\bibitem{hafner2020dreamer} D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, ``Dream to Control: Learning Behaviors by Latent Imagination,'' in ICLR, 2020.

\bibitem{hafner2023dreamerv3} D. Hafner et al., ``Mastering Diverse Domains through World Models,'' arXiv preprint arXiv:2301.04104, 2023.

\bibitem{schrittwieser2020muzero} J. Schrittwieser et al., ``Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model,'' Nature, vol. 588, pp. 604--609, 2020.

% Auxiliary Losses and Self-Supervision
\bibitem{jaderberg2017unreal} M. Jaderberg et al., ``Reinforcement Learning with Unsupervised Auxiliary Tasks,'' in ICLR, 2017.

\bibitem{laskin2020curl} M. Laskin, A. Srinivas, and P. Abbeel, ``CURL: Contrastive Unsupervised Representations for Reinforcement Learning,'' in ICML, 2020.

\bibitem{schwarzer2021spr} M. Schwarzer et al., ``Data-Efficient Reinforcement Learning with Self-Predictive Representations,'' in ICLR, 2021.

% Model-Based Value Expansion
\bibitem{feinberg2018mve} V. Feinberg et al., ``Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning,'' in ICML, 2018.

\bibitem{buckman2018steve} J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee, ``Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion,'' in NeurIPS, 2018.

% Active Inference and Predictive Processing
\bibitem{friston2010free} K. Friston, ``The free-energy principle: a unified brain theory?,'' Nature Reviews Neuroscience, vol. 11, no. 2, pp. 127--138, 2010.

\bibitem{millidge2021expected} B. Millidge, A. Tschantz, and C. L. Buckley, ``Whence the Expected Free Energy?,'' Neural Computation, vol. 33, no. 2, pp. 447--482, 2021.

% RL as Sequence Modeling
\bibitem{chen2021dt} L. Chen et al., ``Decision Transformer: Reinforcement Learning via Sequence Modeling,'' in NeurIPS, 2021.

% Core RL Algorithms
\bibitem{williams1992reinforce} R. J. Williams, ``Simple statistical gradient-following algorithms for connectionist reinforcement learning,'' Machine Learning, vol. 8, no. 3-4, pp. 229--256, 1992.

% RLHF
\bibitem{ouyang2022instructgpt} L. Ouyang et al., ``Training language models to follow instructions with human feedback,'' in NeurIPS, 2022.

\end{thebibliography}

\end{document}
