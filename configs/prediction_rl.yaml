# Phase 2b: Prediction-as-Action with RL
# The agent's action IS its prediction of the next state
# Reward = prediction accuracy (negative MSE)

# Task settings
num_positions: 8
dynamics_type: circular_shift  # or: random_fixed, identity
reward_type: negative_mse      # or: threshold_binary
threshold: 0.1

# Model settings
hidden_dim: 64
num_layers: 2
num_heads: 4
ff_dim: 256

# Training settings
training_mode: rl  # REINFORCE with reward = -MSE
lr: 0.001
total_steps: 10000
batch_size: 32
eval_interval: 500
log_interval: 100

# RL-specific
entropy_coef: 0.01
baseline_decay: 0.99

# Device
device: cuda

# Seed
seed: 42
